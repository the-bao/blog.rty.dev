+++
title = "理解自注意力机制"
date = 2025-10-27T10:00:00+08:00
authors = ["RTY"]
description = "一个门外汉理解自注意力机制的记录"
categories = ["技术","大模型"]
tags = ["llm"]
draft = false
toc = true
math = true    # 启用这篇文章的数学公式
+++

本文仅为帮助门外汉（我）理解自注意力机制的记录

## 名词解释

### 序列转换模型

**序列转换模型**是一类用于将一种序列数据转换为另一种序列数据的神经网络模型。这类模型的关键在于能够理解和生成序列数据，并在输入和输出序列之间建立有效的映射关系。通过这种映射关系，序列转换模型能够在输入文本和目标文本之间找到语义和语法上的对应，从而实现高质量的翻译或生成任务。因此，序列转换模型在自然语言处理（NLP）任务中广泛应用，如机器翻译、语音识别和文本生成等。

典型的序列转换模型通常包括一个**编码器**（encoder）和一个**解码器**（decoder）。编码器负责将输入序列编码成一个固定长度的隐状态表示，而解码器则利用这个隐状态表示生成目标序列。在这些模型中，循环神经网络（RNN）和卷积神经网络（CNN）是从前最常见的架构。然而，2017年以来，基于注意力机制的**Transformer架构**（即该论文介绍的架构）因其并行计算能力和处理长距离依赖关系的优势，成为序列转换任务中的新宠。



### 循环神经网络（RNN）

**循环神经网络**（Recurrent Neural Network，简称RNN）是一种用于处理序列数据的神经网络架构。与传统的前馈神经网络不同，RNN具有环形连接，使得信息能够在网络中循环流动。这种结构使RNN能够保留先前输入的信息，并将其用于当前的输出计算，因此特别适合处理时间序列数据或自然语言处理任务，如语音识别、机器翻译和文本生成。

RNN的关键特性是它的**隐状态**（hidden state），这是一个包含网络在任意时间步的信息的向量。通过隐状态，RNN可以记住前面的输入，从而在处理当前输入时考虑到上下文。然而，传统的RNN在处理长序列时会遇到**梯度消失和梯度爆炸**问题，这限制了其记忆长距离依赖的能力。



### 卷积神经网络（CNN）

**卷积神经网络**（Convolutional Neural Network，简称CNN）是一种专门用于处理具有网格状拓扑结构数据的神经网络架构，如图像和视频。CNN通过引入卷积层和池化层，能够有效地捕捉空间结构中的局部特征，特别适合处理二维图像数据。

CNN的层次结构使其能够从低级特征到高级语义特征逐步提取信息，这种逐层提取和组合特征的方式，使得CNN在计算机视觉任务中取得了显著成功。随着深度学习的发展，CNN的变种如ResNet、Inception等也被广泛应用于各种图像处理任务中。



## 核心结构

### 1.输入部分

- **词嵌入**：将输入的单词转换为向量。
- **位置编码**：这是 Transformer 的关键创新之一。因为模型不再按顺序处理数据，它失去了词序信息。**位置编码**就是给每个词向量加上一个特殊的、表示其位置（如第1个词、第2个词）的向量。这样模型就能知道词的顺序信息。



### 2.编码器

编码器的任务是**理解**输入序列，并将其压缩成一个富含上下文信息的“记忆包”。每一层编码器都包含两个子层：

- **多头自注意力机制**
  - **目的**：找到序列中所有词之间的相互关联程度。
  - **工作原理**：对于序列中的每一个词，它都会生成三个向量：
    - **查询向量（**Q**）**：相当于“我要找什么？”
    - **键向量（K）**：相当于“我有什么内容？”
    - **值向量（V）**：相当于“我的实际信息是什么？”
  - 通过计算一个词的 **Q** 与所有词的 **K** 的相似度（点积），得到一组**注意力分数**。这个分数决定了在理解当前词时，应该**“注意”** 其他词的程度。最后，用这些分数对所有词的 **V** 进行加权求和，得到当前词新的、包含全局上下文信息的表示。
  - **“多头”**：意思是并行地运行多个独立的注意力机制。这允许模型同时关注来自不同“表示子空间”的信息。例如，一个头关注语法关系，另一个头关注指代关系，等等。

- **前馈神经网络**
  - 这是一个简单的全连接网络，对每个位置的向量进行独立且相同的变换。它的作用是进一步处理和整合自注意力层输出的信息。
- **残差连接与层归一化**
  - 每个子层周围都有一个**残差连接**（将子层的输入直接加到输出上），然后进行**层归一化**。这有助于稳定训练，让深层网络更容易优化。



### 3.解码器

解码器的任务是**生成**输出序列。它和编码器很像，但有一些关键区别：

- **掩码多头自注意力**
  - 为了防止在训练时“作弊”（看到未来的词），它被设计为只能关注**当前及之前**的位置。这是通过一个“掩码”来实现的，将未来位置的注意力分数设为负无穷。
- **编码器-解码器注意力层**
  - 这是连接编码器和解码器的桥梁。它的 **Q** 来自解码器的上一层的输出，而 **K** 和 **V** 来自**编码器的最终输出**。
  - 这允许解码器在生成每一个词时，都能有选择地“回顾”输入序列中最相关的部分。
- 同样，它也包含**前馈神经网络**、**残差连接**和**层归一化**。



### 4.输出部分

最后的线性层和 Softmax 层将解码器的输出转换为一个概率分布，表示下一个词是词汇表中每个词的概率。



## 自注意力机制（Self-Attention）

$$
Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
$$



#### 第1步：计算注意力分数 `QK^T`

- **Q, K, V 是什么？**
  - 它们不是三个不同的东西，而是**同一个输入序列**经过三个不同的线性变换后得到的三个表示。
  - **输入**：序列中的每个词都有一个词向量。假设我们有 `n` 个词，每个词向量维度是 `d_model`。那么整个输入就是一个 `n x d_model` 的矩阵。
  - **线性变换**：我们将这个输入矩阵分别乘以三个权重矩阵 `W^Q`, `W^K`, `W^V`。
    - **Q = X \* W^Q** （`W^Q` 的维度是 `d_model x d_k`）
    - **K = X \* W^K** （`W^K` 的维度是 `d_model x d_k`）
    - **V = X \* W^V** （`W^V` 的维度是 `d_model x d_v`）
  - 这样，每个词都得到了三个新的向量：
    - **Query**：表示“我正在寻找什么？”
    - **Key**：表示“我有什么特点？（可供被检索）”
    - **Value**：表示“我真正的信息内容是什么？”
- **QK^T 的含义：**
  - 这一步是计算 **Query** 和 **Key** 的相似度。
  - `Q` 是一个 `n x d_k` 的矩阵，`K^T` 是一个 `d_k x n` 的矩阵。
  - 它们相乘的结果是一个 `n x n` 的矩阵。这个矩阵通常被称为 **注意力分数矩阵**。
  - **这个矩阵的每一行** 对应一个 **Query**（一个词），**每一列** 对应一个 **Key**（一个词）。
  - **矩阵中的每个元素** `(i, j)` 表示第 `i` 个词与第 `j` 个词之间的相关度分数。分数越高，说明在理解第 `i` 个词时，第 `j` 个词越重要。

**比喻**：这就像你（第 `i` 个词的 Query）把图书馆里所有书的索引卡（所有词的 Key）都翻了一遍，并给每张卡打了一个相关分。



#### 第2步：缩放与归一化 `softmax(\frac{...}{\sqrt{d_k}})`

- **缩放：`除以 \sqrt{d_k}`**
  - **目的**：防止点积结果过大。
  - **原因**：当向量维度 `d_k` 很高时，点积的结果可能会变得非常大。这会导致在后续应用 softmax 时，梯度变得非常小（因为 softmax 会将非常大的输入值推向饱和区），不利于模型训练。
  - `\sqrt{d_k}` 是一个经验性的缩放因子，使得点积的结果大小保持在合适的范围内，从而保证训练时梯度的稳定性。
- **归一化：`softmax`**
  - **目的**：将注意力分数转换为概率分布。
  - **作用**：对每一行（即每个 Query）的分数进行 softmax 操作。
  - **效果**：
    1. **所有分数变为正数**。
    2. **每一行的所有分数之和为 1**。
  - 现在，这个 `n x n` 的矩阵变成了 **注意力权重矩阵**。每一行表示一个词对所有词的“注意力分配方案”。

**比喻**：你根据刚才打的相关分，计算出你应该花在每本书上的**时间比例**。比如，书A占50%的阅读时间，书B占30%，书C占20%。这个比例之和是100%。



#### 第3步：加权求和 `... V`

- **含义**：使用归一化后的注意力权重对 **Value** 向量进行加权求和。
- **操作**：注意力权重矩阵 (`n x n`) 乘以 Value 矩阵 (`n x d_v`)。
- **结果**：得到一个 `n x d_v` 的输出矩阵。
  - **这个输出矩阵的每一行**，都是对应位置的词**新的表示向量**。
  - 这个新向量不再是它孤立的原始信息，而是**融入了整个序列中所有词的信息**，并且根据重要性（注意力权重）进行了加权融合。

**比喻**：你按照刚才计算的时间比例（注意力权重），去阅读所有书（Value），从每本书中汲取相应的信息，最后在你的脑子里融合成一个全新的、更丰富的知识体系（输出向量）。



### 总结与直观理解

让我们把整个过程串起来，用一个简单的例子说明：

**句子：** “The animal didn't cross the street because **it** was too tired.”

**任务：** 理解 **“it”** 指代什么。

1. **生成 Q, K, V**：模型为句子中的每个词（"The", "animal", ..., "it", ... "tired"）都生成了一套 Q, K, V。
2. **计算注意力分数（QK^T）**：当模型处理到 "it" 这个词时，它会用 "it" 的 **Query** 向量去和所有词的 **Key** 向量计算点积。我们会发现，"it" 的 Query 与 "animal" 的 Key 分数很高，与 "tired" 的 Key 分数也可能很高，但与 "street" 的 Key 分数很低。
3. **归一化（Softmax）**：将这些分数转换成一个概率分布。可能 "animal" 的权重是 0.6，"tired" 的权重是 0.3，其他词的权重很小。
4. **加权求和（乘以 V）**：模型用这个权重（0.6, 0.3, ...）对所有词的 **Value**（它们实际的含义信息）进行求和。最终，"it" 的新表示向量中，就包含了 60% 的 "animal" 信息和 30% 的 "tired" 信息。
5. **结果**：模型通过这个机制，成功地理解了 "it" 指的是 "animal"，并且它的状态是 "tired"。

这就是自注意力机制的魔力：**它让序列中的每个词都能直接与所有其他词进行“对话”，并根据对话的重要性，动态地构建一个新的、富含上下文的表示。**